import json
import pickle
import uuid
import joblib
import lief
import numpy as np
import pandas as pd
import streamlit as st
from gensim.models import Word2Vec
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import RFE
from sklearn.impute import SimpleImputer
# this library take an string and convert it to the original object that string represent such as list , dict , etc.
import ast

MalwareDetection = open("Pickles/classifier.pickle", "rb")
classifier = joblib.load(MalwareDetection)
w2vmodel = Word2Vec.load('Pickles/word2vec.model')

def predict_malware(data):
    result = classifier.predict(np.array(data).reshape(1, -1))
    return result[0]


def get_raw_dataset(macho_input):
    # Create a raw data set that will contain records describing Mach-O files as JSON objects.
    dataset = pd.DataFrame()
    binary = lief.parse(macho_input)
    binary_json = json.loads(lief.to_json(binary))
    row = pd.Series(binary_json)
    dataset = dataset.append(row, ignore_index=True)
    return dataset


def extract_raw_features(raw_dataset, column):
    raw_features = {}
    for index, value in raw_dataset[column].items():

        value = str(value)

        if value == "nan":
            raw_features[index] = value  # Nan
        else:
            value_type = ast.literal_eval(value)

            if isinstance(value_type, dict):
                if column == 'data_in_code':  # extract the size of all the data embedding blocks for each file.
                    data_block_size = 0
                    for entrie in value_type['entries']:
                        data_block_size = data_block_size + entrie['length']
                    value_type['entries'] = data_block_size

                raw_features[index] = value_type

            elif isinstance(value_type, list):
                if column == 'libraries':  # extract the name of the libraries which the file use.
                    libraries_names = []
                    for lib in value_type:
                        libraries_names.append(lib['name'])
                    raw_features[index] = libraries_names

                if column == 'sections':  # extract the size of all the sections in bytes of the file.
                    sections_size = 0
                    for section in value_type:
                        sections_size = sections_size + section['size']
                    raw_features[index] = sections_size

                if column == 'segments':  # extract information about the protection level for each segment in the file.
                    segments = {}
                    segment_number = 0
                    for segment in value_type:
                        segment_name = segment['name'].replace("__", "")
                        if segment_name not in segments:  # there can be multiple segments with the same name
                            segments[segment_name + "_file_size"] = segment['file_size']
                        else:
                            segments[segment_name + "_file_size"] = segments[segment_name + "_file_size"] + segment[
                                'file_size']

                    raw_features[index] = segments

                if column == 'relocations':  # extract the number of relocations that are present in that file.
                    raw_features[index] = len(value_type)

                if column == 'symbols':  # extract the length of all the symbols in the symbol table.
                    symbols_names = []
                    for symbol in value_type:
                        symbols_names.append(symbol['name'])
                    raw_features[index] = len(symbols_names)

    if column == 'libraries':
        raw_features = pd.DataFrame([raw_features]).T
        raw_features.columns = [column + "_" + "names"]
        # create a new column in the dataframe , which represent the amount of the libraries in each file
        raw_features[column + '_length'] = raw_features[column + "_" + "names"].apply(lambda x: len(x))

    elif column == 'relocations' or column == 'sections' or column == 'symbols':
        raw_features = pd.DataFrame(raw_features, index=[0]).T
        raw_features.columns = [column + "_" + "size"]

    else:
        raw_features = pd.DataFrame(raw_features).T
        raw_features.columns = [column + "_" + col for col in raw_features.columns]

    return raw_features


def get_extracted_features(raw_dataset):
    extracted_features = []
    for column in raw_dataset.columns:
        extracted_features.append(extract_raw_features(raw_dataset, column))

    concatenated = pd.concat(extracted_features, axis="columns")
    concatenated = concatenated.applymap(str)

    for column in concatenated.columns[concatenated.isna().any()].tolist():
        concatenated[column] = concatenated[column].fillna('None')

    return concatenated


def columns_to_numeric(dataset):
    # convert all the elements in the dataframe to numeric, if it is not possible to convert an element to a numeric it will be converted to NaN.
    numeric = dataset.apply(pd.to_numeric, errors='coerce')
    columns = numeric.columns[numeric.isna().sum() < dataset.shape[0]].tolist()

    # after we got the columns that can be a numeric , we convert them in the input dataset.
    dataset[columns] = dataset[columns].apply(pd.to_numeric, errors='coerce')

    return dataset


def compute_relative_offsets(row):
    # Convert addresses to relative offsets
    try:
        value_type = ast.literal_eval(row)
        offsets = np.diff(value_type)
        # Return list of relative offsets
        if len(offsets) == 0:
            return 0
        else:
            return np.mean(offsets)
    except:
        return 0


def header_flags(dataset):
    unique_header_flags = set()
    for flags in dataset['header_flags']:
        flags_list = ast.literal_eval(flags)
        for flag in flags_list:
            unique_header_flags.add(flag)

    return unique_header_flags


def get_uuid_code(row):
    try:
        uuid_bytes = bytes(ast.literal_eval(row))
        uuid_code = uuid.UUID(bytes=uuid_bytes)
        return str(uuid_code)
    except:
        return "None"


def generate_embeddings(dataset, model):
    # convert string representation of list to actual list
    libraries_names = [ast.literal_eval(names) for names in dataset['libraries_names'].tolist()]
    # convert all strings to lowercase and extract library names
    libraries_vocabulary = [[name.lower().split("/")[-1] for name in names] for names in libraries_names]
    # create a list of new words not found in the existing vocabulary
    #new_words = set([word for library in libraries_vocabulary for word in library if word not in model.wv.vocab])
    new_words = set([word for library in libraries_vocabulary for word in library if word not in model.wv.key_to_index])
    if new_words:
        # retrain the model with new words if they exist
        model.build_vocab(libraries_vocabulary, update=True)
        model.train(libraries_vocabulary, total_examples=model.corpus_count, epochs=model.epochs)
    # create embeddings for all libraries in the dataset
    embeddings = []
    for libraries in libraries_vocabulary:
        row_embedding = []
        for token in libraries:
            row_embedding.append(np.array(model.wv[token]))
        embeddings.append(np.array(row_embedding).flatten())
    # find the embedding with the longest length to determine the number of columns
    max_length = max(len(x) for x in embeddings)
    # create a DataFrame with the embeddings as values
    embeddings = pd.DataFrame(embeddings, columns=[f"{i}" for i in range(max_length)])
    embeddings = embeddings.fillna(embeddings.median())
    return embeddings


def fillnone_dataset(dataset):
    for column in dataset.columns[dataset.isna().any()].tolist():
        dataset[column].fillna(value='None', inplace=True)
        return dataset


def dataset_vectorizer(dataset, Word2Vec,object_columns):
    dataset['function_starts_functions'] = dataset['function_starts_functions'].apply(compute_relative_offsets)
    object_columns.remove('function_starts_functions')
    # create a new column for each flag name - onehot encoding
    for flag_name in unique_header_flags:
        dataset[flag_name] = dataset['header_flags'].apply(lambda x: 1 if flag_name in x else 0)

    dataset.drop('header_flags', axis=1, inplace=True)
    object_columns.remove('header_flags')

    # replace the uuid represented by a list of integers with the appropriate unique identifier
    if 'uuid_uuid' in dataset.columns:
        dataset['uuid_uuid'] = dataset['uuid_uuid'].apply(get_uuid_code)
    else:
        dataset['uuid_uuid'] = np.nan
        # creating embedding representation for the library's names using the Word2Vec model, and reducing the dimension by pre-trained PCA algorithm.
    embeddings = generate_embeddings(dataset, Word2Vec)


    dataset = pd.concat([dataset, pd.DataFrame(embeddings)], axis=1).sample(frac=1)
    dataset.drop(['libraries_names'], axis=1, inplace=True)
    object_columns.remove('libraries_names')

    le = LabelEncoder()
    for column in object_columns:
        dataset[column].fillna(value="None", inplace=True)
        dataset[column] = le.fit_transform(dataset[column])

    return dataset

simple_imputer_features = ['code_signature_command',
 'code_signature_command_offset',
 'code_signature_command_size',
 'code_signature_data_hash',
 'code_signature_data_offset',
 'code_signature_data_size',
 'data_in_code_command',
 'data_in_code_command_offset',
 'data_in_code_command_size',
 'data_in_code_data_hash',
 'data_in_code_data_offset',
 'data_in_code_data_size',
 'data_in_code_entries',
 'dyld_info_command',
 'dyld_info_command_offset',
 'dyld_info_command_size',
 'dyld_info_data_hash',
 'dylinker_command',
 'dylinker_command_offset',
 'dylinker_command_size',
 'dylinker_data_hash',
 'dylinker_name',
 'dynamic_symbol_command_command',
 'dynamic_symbol_command_command_offset',
 'dynamic_symbol_command_command_size',
 'dynamic_symbol_command_data_hash',
 'dynamic_symbol_command_external_reference_symbol_offset',
 'dynamic_symbol_command_external_relocation_offset',
 'dynamic_symbol_command_idx_external_define_symbol',
 'dynamic_symbol_command_idx_local_symbol',
 'dynamic_symbol_command_idx_undefined_symbol',
 'dynamic_symbol_command_indirect_symbol_offset',
 'dynamic_symbol_command_local_relocation_offset',
 'dynamic_symbol_command_module_table_offset',
 'dynamic_symbol_command_nb_external_define_symbols',
 'dynamic_symbol_command_nb_external_reference_symbols',
 'dynamic_symbol_command_nb_external_relocations',
 'dynamic_symbol_command_nb_indirect_symbols',
 'dynamic_symbol_command_nb_local_relocations',
 'dynamic_symbol_command_nb_local_symbols',
 'dynamic_symbol_command_nb_module_table',
 'dynamic_symbol_command_nb_toc',
 'dynamic_symbol_command_nb_undefined_symbols',
 'dynamic_symbol_command_toc_offset',
 'function_starts_command',
 'function_starts_command_offset',
 'function_starts_command_size',
 'function_starts_data_hash',
 'function_starts_data_offset',
 'function_starts_data_size',
 'function_starts_functions',
 'header_cpu_subtype',
 'header_cpu_type',
 'header_file_type',
 'header_magic',
 'header_nb_cmds',
 'header_reserved',
 'header_sizeof_cmds',
 'libraries_length',
 'main_command_command',
 'main_command_command_offset',
 'main_command_command_size',
 'main_command_data_hash',
 'main_command_entrypoint',
 'main_command_stack_size',
 'relocations_size',
 'rpath_command',
 'rpath_command_offset',
 'rpath_command_size',
 'rpath_data_hash',
 'rpath_path',
 'sections_size',
 'segments_PAGEZERO_file_size',
 'segments_TEXT_file_size',
 'segments_DATA_file_size',
 'segments_LINKEDIT_file_size',
 'source_version_command',
 'source_version_command_offset',
 'source_version_command_size',
 'source_version_data_hash',
 'source_version_version',
 'symbol_command_command',
 'symbol_command_command_offset',
 'symbol_command_command_size',
 'symbol_command_data_hash',
 'symbol_command_numberof_symbols',
 'symbol_command_strings_offset',
 'symbol_command_strings_size',
 'symbol_command_symbol_offset',
 'symbols_size',
 'uuid_command',
 'uuid_command_offset',
 'uuid_command_size',
 'uuid_data_hash',
 'uuid_uuid',
 'version_min_command',
 'version_min_command_offset',
 'version_min_command_size',
 'version_min_data_hash',
 'version_min_sdk',
 'version_min_version',
 'ALLMODSBOUND',
 'TWOLEVEL',
 'PIE',
 'WEAK_DEFINES',
 'DYLDLINK',
 'HAS_TLV_DESCRIPTORS',
 'NO_HEAP_EXECUTION',
 'BINDS_TO_WEAK',
 'NO_REEXPORTED_DYLIBS',
 'NOUNDEFS',
 '0',
 '1',
 '2',
 '3',
 '4',
 '5',
 '6',
 '7',
 '8',
 '9',
 '10',
 '11',
 '12',
 '13',
 '14',
 '15',
 '16',
 '17',
 '18',
 '19',
 '20',
 '21',
 '22',
 '23',
 '24',
 '25',
 '26',
 '27',
 '28',
 '29',
 '30',
 '31',
 '32',
 '33',
 '34',
 '35',
 '36',
 '37',
 '38',
 '39',
 '40',
 '41',
 '42',
 '43',
 '44',
 '45']


if __name__ == "__main__":

    html_title = """
      <div style= "padding: 35px;width: 110%;   font-family:Tahoma;">
      <h2 style = "font-family:Tahoma; color:#058aff; text-align: center;"> Macho File - Malware Detection </h2>
       </div>
      """
    st.markdown(html_title, unsafe_allow_html=True)

    html_insert = """
      <div style= "padding: 35px;width: 110%;   font-family:Tahoma;">
      <h5 style = "font-family:Tahoma; color:#058aff; text-align: center;"> Please insert a Macho file </h5>
       </div>
      """
    st.markdown(html_insert, unsafe_allow_html=True)

    macho_file = st.file_uploader(label=" ", type=None, accept_multiple_files=False, key=None, help=None,
                                  on_change=None, args=None,
                                  kwargs=None, disabled=False, label_visibility="visible")
    if macho_file is not None:
        macho_file = macho_file.getvalue()
        try:
            raw_dataset = get_raw_dataset(macho_file)
            dataset = get_extracted_features(raw_dataset)
            dataset = columns_to_numeric(dataset)
            unique_header_flags = header_flags(dataset)
            print('unique_header_flags: ', unique_header_flags)
            if 'function_starts_functions' not in dataset:
                dataset['function_starts_functions'] = [""]
    
            object_columns = list(dataset.columns[dataset.dtypes == object])
            sample = dataset_vectorizer(dataset, w2vmodel,object_columns)
            print('Dataset vectorized successfully!')
    
            sample[sample == "None"] = np.nan
            # If model features doesnt exist it creates them
            for i in simple_imputer_features:
                if i not in sample.columns.tolist():
                    sample[i] = np.nan
    
            #imputer
            X = joblib.load('Pickles/X_dataset.pkl')
            sample = sample[simple_imputer_features]
            imputer = joblib.load('Pickles/imputer.pkl')
            print('len:',len(X.columns))
    
            imputer.fit(X)
            sample_imputed = imputer.transform(sample)
            # Standardize the data
            scaler = joblib.load('Pickles/scaler.pkl')
            scaler.fit(X)
            sample_scaled = scaler.transform(sample_imputed)
            selector = joblib.load('Pickles/selector.pkl')
            sample_selected = selector.transform(sample_scaled)
            print('sample scaled:',sample_scaled)
    

            features = sample_selected
    
            m = st.markdown("""
                                <style>
                                div.stButton > button:first-child {
                                      height: 3em;
                                      width: 10em;
                                      position: relative;
                                      margin-left: 300px; 
                                      font-family:Tahoma;
                                }
                                </style>""", unsafe_allow_html=True)
    
            if st.button("Predict"):
                output = predict_malware(features)
                if output == 1:
                    st.markdown(
                        """ <p style = "text-align: center;  font-family:Tahoma; color:blue; ">The model prediction: <a style = "font-weight: bold; color:#cc0000;"> Malware. </a></p>""",
                        unsafe_allow_html=True)
                else:
                    st.markdown(
                        """ <p style = "text-align: center;  font-family:Tahoma; color:blue; ">The model prediction: <a style = "font-weight: bold; color:#0080f0;"> Benign. </a></p>""",
                        unsafe_allow_html=True)

        except:
           st.markdown(
               """ <p style = "text-align: center;  font-family:Tahoma; color:red; "font-weight: bold; "> Unknown format - Can't parse the file, please try an other 'Macho' file</p>""",
               unsafe_allow_html=True)
